# This is a bit lengthy, so bear with me. #gobears
# Clear memory
rm(list = ls()) # clear memory
library(pacman)
p_load(ggplot2,MASS, estimatr, tidyr)
# AER is the standard IV package people use.
# Sample size & number of loops & placeholders
N  <- 1000
numloop <- 1000
b <- matrix(NA, nrow = numloop, ncol = 3) # vector to hold y2 coefficient for each iteration and estimator
se <- matrix(NA, nrow = numloop, ncol = 3) # vector to hold standard error for y2 coefficient for each iteration and estimator
# In this exercise we will generate some data for an endogenous variable and generate
# Some valid instruments. You can play with this setup to study a number of things.
# Step 1: Let's generate some data for an IV setup that has
# one problem rhs variable (y2). In order to make this a problem right hand side variable, we generate a variable that is correlated with
# y2 and omitted from the regression. We call this one y3. This induces the correlation between the error and y2, which violated the
# exogeneity assumption.
# For fun, we also include two exogenous variables (x1 and x2),
# two instruments and a well behaved error terms
# Make some mean for my variables. I choose zero to make it simple.
mu <- matrix(c(0,0,0,0,0,0,0),ncol=1)
# Make the correlation matrix
# There will be seven variable generated.
# y2 and y3 (correlated) - I choose 0.7 for no reason other than I like the number 7.
# x1 and x2 (uncorrelated - but that does not really matter, unless they are too correlated)
# z1 and z2 (these will be my "valid" instrumental variables. They have to be correlated with y2 and uncorrelated with the disturbance). I choose 0.5. No real reason.
# err is out random "structural" disturbance.
# So same (but bigger than last time.)
sigma = matrix(c(
1,0.7,0,0,0.5,0.5,0,
0.7,1,0,0,0,0,0,
0,0,1,0,0,0,0,
0,0,0,1,0,0,0,
0.5,0,0,0,1,0,0,
0.5,0,0,0,0,1,0,
0,0,0,0,0,0,1),nrow=7)
# Now start the Monte Carlo (do this numloop times. You choose numloop. More takes longer.)
for(i in 1:numloop) {
# Create some data
x = as.data.frame(mvrnorm(n=N,mu, sigma))
# Name some things
names(x)[1] <- "y2"
names(x)[2] <- "y3"
names(x)[3] <- "x1"
names(x)[4] <- "x2"
names(x)[5] <- "z1"
names(x)[6] <- "z2"
names(x)[7] <- "err"
# # Generate some true outcomes (I choose alternating ones and -1 ones for my coefficients. Easier to see what is happening when things break down.)
x$y1 = 1 - 1*x$y2 + 1*x$y3 -1 *x$x1 + 1*x$x2 + x$err
# Run an OLS regression of y1 on  y2 and the x1 and x2 (leave out y3). This should be biased. Which way?
# Positive! The coefficient on y3 is positive as is the correlation between y2 and y3. OVB formula strikes again!
mod_1 = lm(y1 ~ 1 + y2 + x1 + x2, data=x)
# record the coefficient on y2 for each loop.
b[i,1] <- mod_1$coefficients[2]
out1 <-summary(mod_1)
# I want to show you the behavior of the standard errors later. So let's record them.
se[i,1]  <- out1$coefficients[2,2]
# Just for fun.
# IV by hand, with just z1 as a single instrument. Run a regression of y2 on z1 and the xs. save the predicted values.
stage_1 <- lm(y2 ~ 1 + x1 + x2 + z1, data = x)
x$y2_hat <- stage_1$fitted.values
# Now run a regression of y1 on the predicted values you just calculated x$y_hat and x1 and x2.
# What do you see?
stage_2 <- lm(y1 ~ 1+ y2_hat + x1 + x2, data=x)
b[i,2] <- stage_2$coefficients[2]
mod_2 <-summary(stage_2)
# I want to show you the behavior of the standard errors later. So let's record them.
se[i,2]  <- mod_2$coefficients[2,2]
# Now use the canned command.
mod_3 <- iv_robust(y1  ~ y2 + x1 + x2| x1 + x2 + z1, data = x, se_type ="classical")
b[i,3] <- mod_3$coefficients[2]
se[i,3] <- mod_3$std.error[2]
}
df <- as.data.frame(b)
df_long <- pivot_longer(df, everything(), names_to = "column", values_to = "value")
coefs_plot <- ggplot(df_long, aes(x = value, color=value, fill = column)) +
scale_color_manual(name = "Correlation (ρ)", values=c("#002676", "#FC9313","#00553A"), labels = c("OLS", "IV by Hand", "2SLS")) +
scale_fill_manual(name = "Correlation (ρ)", values=c("#002676", "#FC9313","#00553A"), labels = c("OLS", "IV by Hand", "2SLS")) +
geom_histogram(aes(y = after_stat(density)),
bins = 100, position = "identity", alpha = 0.35) +
labs(title = "Coefficient Distribution (y1)", x = "Coefficient Values", y = "Density") +
theme_minimal()
print(coefs_plot)
df2 <- as.data.frame(se)
df_long2 <- pivot_longer(df2, everything(), names_to = "column", values_to = "value")
ses_plot <- ggplot(df_long2, aes(x = value, color=value, fill = column)) +
scale_color_manual(name = "Correlation (ρ)", values=c("#002676", "#FC9313","#00553A"), labels = c("OLS", "IV by Hand", "2SLS")) +
scale_fill_manual(name = "Correlation (ρ)", values=c("#002676", "#FC9313","#00553A"), labels = c("OLS", "IV by Hand", "2SLS")) +
geom_histogram(aes(y = after_stat(density)),
bins = 100, position = "identity", alpha = 0.35) +
labs(title = "Standard Error Distribution (y1)", x = "SE Values", y = "Density") +
theme_minimal()
print(ses_plot)
# We are going to replicate Carpenter and Dobkin one the effects of minimum drinking age on
# mortality - This is the exercise conducted by Philip Leppert.
rm(list = ls()) # clear memory
# Replace this with whatever your directory is.
setwd("/Users/auffhammer/Library/CloudStorage/Dropbox/06_Teaching/MACSS/2025/COMPSS_201_2025/lecture_10")
#Jupyter Path is
#setwd("~/COMPSS_201_2025/lecture_10")
library(pacman)
p_load(dplyr,ggplot2,rddtools,magrittr)
# Read in data
carpenter_dobkin_2009 <- read.csv("dobkin.csv")
# At first, we have to compute a dummy variable (threshold), indicating whether an individual is
# below or above the cutoff. The dummy is equal to zero for observations below and equal to one
# for observations above the cutoff of 21 years.
#Then I am specifying a linear model with function lm() to regress all deaths
#per 100.000 (all) on the threshold dummy and the respondents’ age which is
#centered around the threshold value of age (21 years).
#This is done with function I() by subtracting the cutoff from each age bin.
carpenter_dobkin_2009 %>%
ggplot(aes(x = agecell, y = all)) +
geom_point() +
geom_vline(xintercept = 21, color = "#ff0091", linewidth = 1, linetype = "dashed") +
annotate("text", x = 20.4, y = 105, label = "Minimum Drinking Age") +
labs(y = "Mortality rate (per 100.000)",
x = "Age (binned)") +
theme_classic(base_size = 14)
lm_same_slope <- carpenter_dobkin_2009 %>%
mutate(threshold = ifelse(agecell >= 21, 1, 0)) %$%
lm(all ~ threshold + I(agecell - 21))
# Output of the model.
summary(lm_same_slope)
rdd_data(y = carpenter_dobkin_2009$all,
x = carpenter_dobkin_2009$agecell,
cutpoint = 21) %>%
rdd_reg_lm(slope = "same") %>%
summary()
# We are going to replicate Carpenter and Dobkin one the effects of minimum drinking age on
# mortality - This is the exercise conducted by Philip Leppert.
rm(list = ls()) # clear memory
# Replace this with whatever your directory is.
setwd("/Users/auffhammer/Library/CloudStorage/Dropbox/06_Teaching/MACSS/2025/COMPSS_201_2025/lecture_10")
#Jupyter Path is
#setwd("~/COMPSS_201_2025/lecture_10")
library(pacman)
p_load(dplyr,ggplot2,rddtools,magrittr)
# Read in data
carpenter_dobkin_2009 <- read.csv("dobkin.csv")
# At first, we have to compute a dummy variable (threshold), indicating whether an individual is
# below or above the cutoff. The dummy is equal to zero for observations below and equal to one
# for observations above the cutoff of 21 years.
#Then I am specifying a linear model with function lm() to regress all deaths
#per 100.000 (all) on the threshold dummy and the respondents’ age which is
#centered around the threshold value of age (21 years).
#This is done with function I() by subtracting the cutoff from each age bin.
carpenter_dobkin_2009 %>%
ggplot(aes(x = agecell, y = all)) +
geom_point() +
geom_vline(xintercept = 21, color = "#ff0091", linewidth = 1, linetype = "dashed") +
annotate("text", x = 20.4, y = 105, label = "Minimum Drinking Age") +
labs(y = "Mortality rate (per 100.000)",
x = "Age (binned)") +
theme_classic(base_size = 14)
#At first, we have to compute a dummy variable (threshold),
# indicating whether an individual is below or above the cutoff.
# The dummy is equal to zero for observations below and equal to one for
# observations aboev the cutoff of 21 years. Then I am specifying a linear model
# with function lm() to regress all deaths per 100.000 (all) on the threshold dummy
# and the respondents’ age which is centered around the threshold value of age (21 years).
# This is done with function I() by substracting the cutoff from each age bin.
lm_same_slope <- carpenter_dobkin_2009 %>%
mutate(threshold = ifelse(agecell >= 21, 1, 0)) %$%
lm(all ~ threshold + I(agecell - 21))
# Output of the model.
summary(lm_same_slope)
#There is an alternative approach by using R package rddtools which contains
# various functions related to applying the RDD. Within function rdd_reg_lm()
# I am using the argument slope = "same" to achieve the same result with the previous approach.
rdd_data(y = carpenter_dobkin_2009$all,
x = carpenter_dobkin_2009$agecell,
cutpoint = 21) %>%
rdd_reg_lm(slope = "same") %>%
summary()
install.packages("rddtools")
# We are going to replicate Carpenter and Dobkin one the effects of minimum drinking age on
# mortality - This is the exercise conducted by Philip Leppert.
rm(list = ls()) # clear memory
# Replace this with whatever your directory is.
setwd("/Users/auffhammer/Library/CloudStorage/Dropbox/06_Teaching/MACSS/2025/COMPSS_201_2025/lecture_10")
#Jupyter Path is
#setwd("~/COMPSS_201_2025/lecture_10")
library(pacman)
p_load(dplyr,ggplot2,rdrobust,magrittr)
# Read in data
carpenter_dobkin_2009 <- read.csv("dobkin.csv")
# At first, we have to compute a dummy variable (threshold), indicating whether an individual is
# below or above the cutoff. The dummy is equal to zero for observations below and equal to one
# for observations above the cutoff of 21 years.
#Then I am specifying a linear model with function lm() to regress all deaths
#per 100.000 (all) on the threshold dummy and the respondents’ age which is
#centered around the threshold value of age (21 years).
#This is done with function I() by subtracting the cutoff from each age bin.
carpenter_dobkin_2009 %>%
ggplot(aes(x = agecell, y = all)) +
geom_point() +
geom_vline(xintercept = 21, color = "#ff0091", linewidth = 1, linetype = "dashed") +
annotate("text", x = 20.4, y = 105, label = "Minimum Drinking Age") +
labs(y = "Mortality rate (per 100.000)",
x = "Age (binned)") +
theme_classic(base_size = 14)
#At first, we have to compute a dummy variable (threshold),
# indicating whether an individual is below or above the cutoff.
# The dummy is equal to zero for observations below and equal to one for
# observations aboev the cutoff of 21 years. Then I am specifying a linear model
# with function lm() to regress all deaths per 100.000 (all) on the threshold dummy
# and the respondents’ age which is centered around the threshold value of age (21 years).
# This is done with function I() by substracting the cutoff from each age bin.
lm_same_slope <- carpenter_dobkin_2009 %>%
mutate(threshold = ifelse(agecell >= 21, 1, 0)) %$%
lm(all ~ threshold + I(agecell - 21))
# Output of the model.
summary(lm_same_slope)
#There is an alternative approach by using R package rddtools which contains
# various functions related to applying the RDD. Within function rdd_reg_lm()
# I am using the argument slope = "same" to achieve the same result with the previous approach.
rdd_data(y = carpenter_dobkin_2009$all,
x = carpenter_dobkin_2009$agecell,
cutpoint = 21) %>%
rdd_reg_lm(slope = "same") %>%
summary()
# We are going to replicate Carpenter and Dobkin one the effects of minimum drinking age on
# mortality - This is the exercise conducted by Philip Leppert.
rm(list = ls()) # clear memory
# Replace this with whatever your directory is.
setwd("/Users/auffhammer/Library/CloudStorage/Dropbox/06_Teaching/MACSS/2025/COMPSS_201_2025/lecture_10")
#Jupyter Path is
#setwd("~/COMPSS_201_2025/lecture_10")
library(pacman)
p_load(dplyr,ggplot2,rdrobust,magrittr)
# Read in data
carpenter_dobkin_2009 <- read.csv("dobkin.csv")
# At first, we have to compute a dummy variable (threshold), indicating whether an individual is
# below or above the cutoff. The dummy is equal to zero for observations below and equal to one
# for observations above the cutoff of 21 years.
#Then I am specifying a linear model with function lm() to regress all deaths
#per 100.000 (all) on the threshold dummy and the respondents’ age which is
#centered around the threshold value of age (21 years).
#This is done with function I() by subtracting the cutoff from each age bin.
carpenter_dobkin_2009 %>%
ggplot(aes(x = agecell, y = all)) +
geom_point() +
geom_vline(xintercept = 21, color = "#ff0091", linewidth = 1, linetype = "dashed") +
annotate("text", x = 20.4, y = 105, label = "Minimum Drinking Age") +
labs(y = "Mortality rate (per 100.000)",
x = "Age (binned)") +
theme_classic(base_size = 14)
#At first, we have to compute a dummy variable (threshold),
# indicating whether an individual is below or above the cutoff.
# The dummy is equal to zero for observations below and equal to one for
# observations aboev the cutoff of 21 years. Then I am specifying a linear model
# with function lm() to regress all deaths per 100.000 (all) on the threshold dummy
# and the respondents’ age which is centered around the threshold value of age (21 years).
# This is done with function I() by substracting the cutoff from each age bin.
lm_same_slope <- carpenter_dobkin_2009 %>%
mutate(threshold = ifelse(agecell >= 21, 1, 0)) %$%
lm(all ~ threshold + I(agecell - 21))
# Output of the model.
summary(lm_same_slope)
rd_out <- rdrobust(y = y, x = x, c = 21, p = 1, kernel = "triangular")
View(carpenter_dobkin_2009)
View(carpenter_dobkin_2009)
rd_out <- rdrobust(y = carpenter_dobkin$ all, x = carpenter_dobkin$agecell, c = 21, p = 1, kernel = "triangular")
View(carpenter_dobkin_2009)
View(carpenter_dobkin_2009)
rd_out <- rdrobust(y = carpenter_dobkin_2009$ all, x = carpenter_dobkin_2009$agecell, c = 21, p = 1, kernel = "triangular")
summary(rd_out)
# We are going to replicate Carpenter and Dobkin one the effects of minimum drinking age on
# mortality - This is the exercise conducted by Philip Leppert.
rm(list = ls()) # clear memory
# Replace this with whatever your directory is.
setwd("/Users/auffhammer/Library/CloudStorage/Dropbox/06_Teaching/MACSS/2025/COMPSS_201_2025/lecture_10")
#Jupyter Path is
#setwd("~/COMPSS_201_2025/lecture_10")
library(pacman)
p_load(dplyr,ggplot2,rdrobust,magrittr)
# Read in data
carpenter_dobkin_2009 <- read.csv("dobkin.csv")
# At first, we have to compute a dummy variable (threshold), indicating whether an individual is
# below or above the cutoff. The dummy is equal to zero for observations below and equal to one
# for observations above the cutoff of 21 years.
#Then I am specifying a linear model with function lm() to regress all deaths
#per 100.000 (all) on the threshold dummy and the respondents’ age which is
#centered around the threshold value of age (21 years).
#This is done with function I() by subtracting the cutoff from each age bin.
carpenter_dobkin_2009 %>%
ggplot(aes(x = agecell, y = all)) +
geom_point() +
geom_vline(xintercept = 21, color = "#ff0091", linewidth = 1, linetype = "dashed") +
annotate("text", x = 20.4, y = 105, label = "Minimum Drinking Age") +
labs(y = "Mortality rate (per 100.000)",
x = "Age (binned)") +
theme_classic(base_size = 14)
#At first, we have to compute a dummy variable (threshold),
# indicating whether an individual is below or above the cutoff.
# The dummy is equal to zero for observations below and equal to one for
# observations aboev the cutoff of 21 years. Then I am specifying a linear model
# with function lm() to regress all deaths per 100.000 (all) on the threshold dummy
# and the respondents’ age which is centered around the threshold value of age (21 years).
# This is done with function I() by substracting the cutoff from each age bin.
lm_same_slope <- carpenter_dobkin_2009 %>%
mutate(threshold = ifelse(agecell >= 21, 1, 0)) %$%
lm(all ~ threshold + I(agecell - 21))
# Output of the model.
summary(lm_same_slope)
rd_out <- rdrobust(y = carpenter_dobkin_2009$ all, x = carpenter_dobkin_2009$agecell, c = 21, p = 1, kernel = "uniform)
summary(rd_out)
rd_out <- rdrobust(y = carpenter_dobkin_2009$ all, x = carpenter_dobkin_2009$agecell, c = 21, p = 1, kernel = "uniform")
rd_out <- rdrobust(y = carpenter_dobkin_2009$ all, x = carpenter_dobkin_2009$agecell, c = 21, p = 1, kernel = "uniform")
summary(rd_out)
rd_out <- rdrobust(y = carpenter_dobkin_2009$ all, x = carpenter_dobkin_2009$agecell, , h=100, c = 21, p = 1, kernel = "uniform")
summary(rd_out)
summary(lm_same_slope)
rm(list = ls()) # clear memory
setwd("/Users/auffhammer/Library/CloudStorage/Dropbox/06_Teaching/MACSS/2025/COMPSS_201_2025/problem_set_2")
songs <- read.csv("song_data.csv")
setwd("/Users/auffhammer/Library/CloudStorage/Dropbox/06_Teaching/MACSS/2025/COMPSS_201_2025/problem_set_2")
songs <- read.csv("song_data_2025.csv")
setwd("/Users/auffhammer/Library/CloudStorage/Dropbox/06_Teaching/MACSS/2025/COMPSS_201_2025/problem_set_2")
songs <- read.csv("song_data_2025.csv")
View(songs)
View(songs)
songs2 <- songs[c(3,4,5,6,7, 14)]
ggpairs(songs2)
# We are going to replicate Carpenter and Dobkin one the effects of minimum drinking age on
# mortality - This is the exercise conducted by Philip Leppert.
rm(list = ls()) # clear memory
# Replace this with whatever your directory is.
setwd("/Users/auffhammer/Library/CloudStorage/Dropbox/06_Teaching/MACSS/2025/COMPSS_201_2025/lecture_10")
#Jupyter Path is
#setwd("~/COMPSS_201_2025/lecture_10")
library(pacman)
p_load(dplyr,ggplot2,rdrobust,magrittr)
# Read in data
carpenter_dobkin_2009 <- read.csv("dobkin.csv")
# At first, we have to compute a dummy variable (threshold), indicating whether an individual is
# below or above the cutoff. The dummy is equal to zero for observations below and equal to one
# for observations at or abive above the cutoff of 21 years.
# Then I am specifying a linear model with function lm() to regress all deaths
# per 100.000 (all) on the threshold dummy and the respondents’ age which is
# centered around the threshold value of age (21 years).
# This is done with function I() by subtracting the cutoff from each age bin.
carpenter_dobkin_2009 %>%
ggplot(aes(x = agecell, y = all)) +
geom_point() +
geom_vline(xintercept = 21, color = "#ff0091", linewidth = 1, linetype = "dashed") +
annotate("text", x = 20.4, y = 105, label = "Minimum Drinking Age") +
labs(y = "Mortality rate (per 100.000)",
x = "Age (binned)") +
theme_classic(base_size = 14)
lm_same_slope <- carpenter_dobkin_2009 %>%
mutate(threshold = ifelse(agecell >= 21, 1, 0)) %$%
lm(all ~ threshold + I(agecell - 21))
# Output of the model.
summary(lm_same_slope)
rd_out <- rdrobust(y = carpenter_dobkin_2009$ all, x = carpenter_dobkin_2009$agecell, , h=100, c = 21, p = 1, kernel = "uniform")
summary(rd_out)
# Do the same thing, but different slopes on each side. Do it by hand
lm_different_slope <- carpenter_dobkin_2009 %>%
mutate(threshold = ifelse(agecell >= 21, 1, 0)) %$%
lm(all ~ threshold + I(agecell - 21) + threshold:I(agecell - 21))
summary(lm_different_slope)
lm_same_slope <- carpenter_dobkin_2009 %>%
mutate(threshold = ifelse(agecell >= 21, 1, 0)) %$%
lm(all ~ threshold + I(agecell - 21))
# Output of the model.
summary(lm_same_slope)
lm_different_slope <- carpenter_dobkin_2009 %>%
mutate(threshold = ifelse(agecell >= 21, 1, 0)) %$%
lm(all ~ threshold + I(agecell - 21) + threshold:I(agecell - 21))
summary(lm_different_slope)
carpenter_dobkin_2009 %>%
select(agecell, all) %>%
mutate(threshold = as.factor(ifelse(agecell >= 21, 1, 0))) %>%
ggplot(aes(x = agecell, y = all, color = threshold)) +
geom_point() +
geom_smooth(method = "lm", se = FALSE) +
scale_color_brewer(palette = "Accent") +
guides(color = FALSE) +
geom_vline(xintercept = 21, color = "red",
size = 1, linetype = "dashed") +
labs(y = "Mortality rate (per 100.000)",
x = "Age (binned)")+
theme_classic(base_size = 14)
# Now quadratic by hand
lm_quadratic <- carpenter_dobkin_2009 %>%
mutate(threshold = ifelse(agecell >= 21, 1, 0)) %$%
lm(all ~ threshold + I(agecell - 21) + I((agecell -21)^2) + threshold:I(agecell - 21) +
threshold:I((agecell - 21)^2))
summary(lm_quadratic)
# Now visualize
carpenter_dobkin_2009 %>%
select(agecell, all) %>%
mutate(threshold = as.factor(ifelse(agecell >= 21, 1, 0))) %>%
ggplot(aes(x = agecell, y = all, color = threshold)) +
geom_point() +
geom_smooth(method = "lm",
formula = y ~ x + I(x ^ 2),
se = FALSE) +
scale_color_brewer(palette = "Accent") +
guides(color = FALSE) +
geom_vline(xintercept = 21, color = "red",
size = 1, linetype = "dashed") +
labs(y = "Mortality rate (per 100.000)",
x = "Age (binned)")+
theme_classic(base_size = 14)
# Now let's limit the sample! Only folks between 20 and 22!
lm_sensitivity <- carpenter_dobkin_2009 %>%
filter(agecell >= 20 & agecell <= 22) %>%
mutate(threshold = ifelse(agecell >= 21, 1, 0)) %$%
lm(all ~ threshold + I(agecell - 21) + threshold:I(agecell - 21))
summary(lm_sensitivity)
carpenter_dobkin_2009 %>%
filter(agecell >= 20 & agecell <= 22) %>%
select(agecell, all) %>%
mutate(threshold = as.factor(ifelse(agecell >= 21, 1, 0))) %>%
ggplot(aes(x = agecell, y = all, color = threshold)) +
geom_point() +
geom_smooth(method = "lm", se = FALSE) +
scale_color_brewer(palette = "Accent") +
guides(color = FALSE) +
geom_vline(xintercept = 21, color = "red",
size = 1, linetype = "dashed") +
labs(y = "Mortality rate (per 100.000)",
x = "Age (binned)")+
theme_classic(base_size = 14)
rdd_data(y = carpenter_dobkin_2009$all,
x = carpenter_dobkin_2009$agecell,
cutpoint = 21) %>%
rdd_reg_lm(slope = "separate", order = 13) %>%
summary()
rm(list = ls()) # clear memory
setwd("/Users/auffhammer/Library/CloudStorage/Dropbox/06_Teaching/MACSS/2025/COMPSS_201_2025/lecture_10")
#Jupyter Path is
#setwd("~/COMPSS_201_2025/lecture_10")
set.seed(22092008) # set random number generator seed
library(pacman)
p_load(tidyverse,broom,rdrobust,estimatr,modelsummary)
# This is an excellent example from Andrew Weiss:
# hypothetical example, students take an entrance exam
# at the beginning of a school year.
# Those who score 70 or below are automatically enrolled
# in a free tutoring program and receive assistance throughout the year.
# At the end of the school year, students take a final test,
# or exit exam (with a maximum of 100 points) to measure how much
# they learned overall. Remember, this is a hypothetical example
# and tests like this don’t really exist
tutoring <- read_csv("tutoring_program_fuzzy.csv")
# Show that some people scored higher on the entrance exam and somehow used tutoring,
# or that some people scored below the threshold but didn’t participate in the program,
# either because they’re never-takers, or because they fell through bureaucratic cracks.
ggplot(tutoring, aes(x = entrance_exam, y = tutoring_text, color = entrance_exam <= 70)) +
# Make points small and semi-transparent since there are lots of them
geom_point(size = 1.5, alpha = 0.5,
position = position_jitter(width = 0, height = 0.25, seed = 1234)) +
# Add vertical line
geom_vline(xintercept = 70) +
# Add labels
labs(x = "Entrance exam score", y = "Participated in tutoring program") +
# Turn off the color legend, since it's redundant
guides(color = "none")
# You can generate a table showing these numbers
tutoring %>%
group_by(tutoring, entrance_exam <= 70) %>%
summarize(count = n()) %>%
group_by(tutoring) %>%
mutate(prop = count / sum(count))
# Visualize the gap:
ggplot(tutoring, aes(x = entrance_exam, y = exit_exam, color = tutoring)) +
geom_point(size = 1, alpha = 0.5) +
# Add a line based on a linear model for the people scoring less than 70
geom_smooth(data = filter(tutoring, entrance_exam <= 70), method = "lm") +
# Add a line based on a linear model for the people scoring 70 or more
geom_smooth(data = filter(tutoring, entrance_exam > 70), method = "lm") +
geom_vline(xintercept = 70) +
labs(x = "Entrance exam score", y = "Exit exam score", color = "Used tutoring")
tutoring_with_bins <- tutoring %>%
mutate(exam_binned = cut(entrance_exam, breaks = seq(0, 100, 5))) %>%
# Group by each of the new bins and tutoring status
group_by(exam_binned, tutoring) %>%
# Count how many people are in each test bin + used/didn't use tutoring
summarize(n = n()) %>%
# Make this summarized data wider so that there's a column for tutoring and no tutoring
pivot_wider(names_from = "tutoring", values_from = "n", values_fill = 0) %>%
rename(tutor_yes = `TRUE`, tutor_no = `FALSE`) %>%
# Find the probability of tutoring in each bin by taking
# the count of yes / count of yes + count of no
mutate(prob_tutoring = tutor_yes / (tutor_yes + tutor_no))
# Plot this puppy
ggplot(tutoring_with_bins, aes(x = exam_binned, y = prob_tutoring)) +
geom_col() +
geom_vline(xintercept = 8.5) +
labs(x = "Entrance exam score", y = "Proportion of people participating in program")
# Let's make an instrument and center this puppy
tutoring_centered <- tutoring %>%
mutate(entrance_centered = entrance_exam - 70,
below_cutoff = entrance_exam <= 70)
tutoring_centered
# Now we have a new column named below_cutoff that we’ll use as an instrument. Most of the time this will be the same as the tutoring column, since most people are compliers. But some people didn’t comply, like person 8 here who was not below the cutoff but still used the tutoring program.
# Let's pretend this cutoff is sharp.
model_sans_instrument <- lm(exit_exam ~ entrance_centered + tutoring,
data = filter(tutoring_centered,
entrance_centered >= -10 &
entrance_centered <= 10))
tidy(model_sans_instrument)
model_fuzzy <- iv_robust(
exit_exam ~ entrance_centered + tutoring | entrance_centered + below_cutoff,
data = filter(tutoring_centered, entrance_centered >= -10 & entrance_centered <= 10)
)
tidy(model_fuzzy)
