# works for continuous variables. Max argued that this also works for proportions!
# After all a proportion is just the average of a binary outcome. Let's
# show this.
rm(list = ls())
set.seed(22092008)
N <- 1000000   # population size
n <- 1000      # sample size
numloop <- 10000 # number of samples
p <- 0.001      # true proportion
# Generate population
r <- rbinom(n = N, size = 1, prob = p)
sig2 <- p * (1 - p)
# Plot population
hist(r, prob = TRUE,
main = "Population: Bernoulli(0.35)",
xlab = "Outcome", col = "lightblue")
readline(prompt="Press [enter] to continue")
# Placeholder for sample means
g <- numeric(numloop)
# Monte Carlo loop
for (i in 1:numloop) {
tmp <- sample(r, n, replace = FALSE)
g[i] <- mean(tmp)
}
# Histogram of sample means
hist(g, prob = TRUE, breaks = 50,
main = "Sampling Distribution of Sample Proportion",
xlab = "Sample Proportion", col = "skyblue")
# Overlay normal curve
curve(dnorm(x, mean = p, sd = sqrt(sig2/n)),
col = "red", lwd = 2, add = TRUE)
# Show comparison
cat("Theoretical mean:", p, "\n")
cat("Empirical mean:", mean(g), "\n")
cat("Theoretical SE:", sqrt(sig2/n), "\n")
cat("Empirical SE:", sd(g), "\n")
# Lecture 3.
# OK. Let's start iwth a warmup. Last time we showed that the Central Limit Theorem
# works for continuous variables. Max argued that this also works for proportions!
# After all a proportion is just the average of a binary outcome. Let's
# show this.
rm(list = ls())
set.seed(22092008)
N <- 1000000   # population size
n <- 400      # sample size
numloop <- 10000 # number of samples
p <- 0.05      # true proportion
# Generate population
r <- rbinom(n = N, size = 1, prob = p)
sig2 <- p * (1 - p)
# Plot population
hist(r, prob = TRUE,
main = "Population: Bernoulli(0.35)",
xlab = "Outcome", col = "lightblue")
readline(prompt="Press [enter] to continue")
# Placeholder for sample means
g <- numeric(numloop)
# Monte Carlo loop
for (i in 1:numloop) {
tmp <- sample(r, n, replace = FALSE)
g[i] <- mean(tmp)
}
# Histogram of sample means
hist(g, prob = TRUE, breaks = 50,
main = "Sampling Distribution of Sample Proportion",
xlab = "Sample Proportion", col = "skyblue")
# Overlay normal curve
curve(dnorm(x, mean = p, sd = sqrt(sig2/n)),
col = "red", lwd = 2, add = TRUE)
# Show comparison
cat("Theoretical mean:", p, "\n")
cat("Empirical mean:", mean(g), "\n")
cat("Theoretical SE:", sqrt(sig2/n), "\n")
cat("Empirical SE:", sd(g), "\n")
#parameter.``
rm(list = ls()) # clear memory
library("dplyr") # some data wrangling tools we like.
set.seed(22092008) # set random number generator seed
N<-1000000 # population size
n <- 1000 # sample size (for calculation of mean)
df = n-1 # we are calculating a mean, so df = n-1
# We will first do this once. Then repeat it {numloop} times.
numloop <- 10000 # number of draws
# Placeholder
g <- numeric(numloop)
gtmp <-numeric(numloop)
se <-numeric(numloop)
cil <-numeric(numloop)
ciu <-numeric(numloop)
cheese <-numeric(numloop)
# Set Type 1 Error Probability
alpha <- 0.05
a2 <-0.5*alpha # claculate alpha/2
# # Let's use normally distributed data
sig2 <- 1 # variance for population
mu <- 0 # mean of population
r <- rnorm(N, mean=mu,sd=sqrt(sig2))
# Plot population
hist(r,prob=TRUE,breaks = 50,main = "Raw data")
tmp <-sample(r, n, replace = FALSE, prob = NULL)
gtmp <- mean(tmp)
se <-sqrt(var(tmp)/n)
cil <- gtmp+qt(0.5*alpha,n-1)*se
ciu <- gtmp-qt(0.5*alpha,n-1)*se
cheese <- between(mu,cil,ciu)
print(cil)
print(ciu)
print(cheese)
#Now let's get to the second question about Confidence Intervals.
# Max said ``We are 95% confident that the true consumption of participants
# lies between 23.78 and 28.23 kWh per day.
# I think there is commonly an issue with the use of the word ``confident``.
# This is standard usage found everywhere, but often gets confused with the word
# ``probability``. This is a discussion in Statistics that is old and still a
# sore spot for many. But the best way to think about a confidence interval
# in your mind is what the asked of the question proposed. `
# `If I calculated a CI  1000 times on different random samples from the
# population, 95% of the time, the Confidence Interval would contain the true
#parameter.``
rm(list = ls()) # clear memory
library("dplyr") # some data wrangling tools we like.
set.seed(22092008) # set random number generator seed
N<-1000000 # population size
n <- 1000 # sample size (for calculation of mean)
df = n-1 # we are calculating a mean, so df = n-1
# We will first do this once. Then repeat it {numloop} times.
numloop <- 10000 # number of draws
# Placeholder
g <- numeric(numloop)
gtmp <-numeric(numloop)
se <-numeric(numloop)
cil <-numeric(numloop)
ciu <-numeric(numloop)
cheese <-numeric(numloop)
# Set Type 1 Error Probability
alpha <- 0.05
a2 <-0.5*alpha # claculate alpha/2
# # Let's use normally distributed data
sig2 <- 1 # variance for population
mu <- 0 # mean of population
r <- rnorm(N, mean=mu,sd=sqrt(sig2))
# Plot population
hist(r,prob=TRUE,breaks = 50,main = "Raw data")
# Let us calculate a confidence interval for a single sample we drew.
tmp <-sample(r, n, replace = FALSE, prob = NULL)
gtmp <- mean(tmp)
se <-sqrt(var(tmp)/n)
cil <- gtmp+qt(0.5*alpha,n-1)*se
ciu <- gtmp-qt(0.5*alpha,n-1)*se
cheese <- between(mu,cil,ciu)
print(cil)
print(ciu)
print(cheese)
# Now let's do this numloop times
for(i in 1:numloop) {
tmp <-sample(r, n, replace = TRUE, prob = NULL)
gtmp[i] <- mean(tmp)
se[i] <-sqrt(var(tmp)/n)
cil[i] <- gtmp[i]+qt(a2,df)*se[i]
ciu[i] <- gtmp[i]-qt(a2,df)*se[i]
cheese[i] <- mu<cil[i] | mu>ciu[i]
}
print(round(mean(cheese),2))
# Lecture 4
# Let's do some testing of differences in means!
# and then some regression stuff.
# The repo also contains a knitr version of this file, which makes nice pdf files.....
rm(list = ls())
set.seed(22092008)
library(pacman)
# ggplot makes pretty graphs. dplyr is awesome. More later.
p_load(ggplot2, dplyr, crosstable, flextable, plyr)
n <- 1000 # Sample Size
mu <- c(0, 0,0)
# a <- 0.5 #Gender Income Covariance
# b <- 0.1 #Gender Insurance Covariance
# c <- 0.8 #Income Insurance
# If insurance were randomized
a <- 0.5 # Set to 0.5 as default
b <- 0.0 # Set to 0.1 as default
c <- 0.0 # Set to 0.8 as default
# Some betas for later
b1 <-1 #Gender Beta
b2 <-5 # Income Beta
b3 <-3 #Insurance
shifter <- 30
Sigma <- matrix(c(1, a, b, a, 1, c,b, c, 1), nrow=3)
data = mvrnorm(n, mu, Sigma, empirical=FALSE)
# Lecture 4
# Let's do some testing of differences in means!
# and then some regression stuff.
# The repo also contains a knitr version of this file, which makes nice pdf files.....
rm(list = ls())
set.seed(22092008)
library(pacman)
# ggplot makes pretty graphs. dplyr is awesome. More later.
p_load(ggplot2, dplyr, crosstable, flextable, plyr, MASS)
n <- 1000 # Sample Size
mu <- c(0, 0,0)
# a <- 0.5 #Gender Income Covariance
# b <- 0.1 #Gender Insurance Covariance
# c <- 0.8 #Income Insurance
# If insurance were randomized
a <- 0.5 # Set to 0.5 as default
b <- 0.0 # Set to 0.1 as default
c <- 0.0 # Set to 0.8 as default
# Some betas for later
b1 <-1 #Gender Beta
b2 <-5 # Income Beta
b3 <-3 #Insurance
shifter <- 30
Sigma <- matrix(c(1, a, b, a, 1, c,b, c, 1), nrow=3)
data = mvrnorm(n, mu, Sigma, empirical=FALSE)
Gender = data[, 1]  # standard normal (mu=0, sd=1)
Income = data[, 2]  # standard normal (mu=0, sd=1)
Insurance= data[, 3]  # standard normal (mu=0, sd=1)
# Gender and income should be binary
Ins <- Insurance>0
Gend <- Gender>0
cor(Ins,Gend)
cor(Ins,Income)
cor(Gend,Income)
# We are going to generate some arbitrary Health Index
Health <- shifter +  rnorm(n,mean=0,sd=1) + b1*Gend + b2*Income + b3* Ins
# Let's do a manual comparison of Health Across the Insured and Uninsured.
mydata <- data.frame(Income, Gend, Health, Ins)
# Calculate Means by Group (using ddply).
mu <- ddply(mydata, "Ins", summarise, grp.mean=mean(Health))
# Plot my health outcome by Insurance Status in Pretty Graph
ggplot(mydata, aes(x=Health, color=Ins, fill=Ins)) +
scale_color_manual(values=c("#002676", "#FC9313")) +
scale_fill_manual(values=c("#002676", "#FC9313")) +
geom_histogram(alpha=0.1, position="identity", bins=50)+
geom_vline(data=mu, aes(xintercept=grp.mean, color=Ins),
linetype="dashed") +
labs(title="Health Outcomes by Insurance Status",x="Health Index", y = "Count")+
theme_classic()
# Let's compare across treatment
# First - do this by hand. Difference in means. Unknown and uneuqal variances.
M1 <- mean(mydata[Ins == 'TRUE', 'Health'])
M2 <- mean(mydata[Ins == 'FALSE', 'Health'])
n1 <- sum(Ins)
n2 <- n-n1
V1 <- var(mydata[Ins == 'TRUE', 'Health'])
V2 <- var(mydata[Ins == 'FALSE', 'Health'])
S <- sqrt((V1 / n1) + (V2 / n2))
statistic <- (M1 - M2 - 0) / S
print(statistic)
t_health <- t.test(Health ~ Ins)
print(t_health)
t_inc <- t.test(Income ~ Ins)
print(t_inc)
my_test_args=crosstable_test_args(show_method=FALSE)
ft1 <- crosstable(mydata,by="Ins", test=TRUE, funs=c(mean=mean),test_args=my_test_args) %>%
as_flextable()
print (ft1)
# Lecture 4
# Let's do some testing of differences in means!
# and then some regression stuff.
# The repo also contains a knitr version of this file, which makes nice pdf files.....
rm(list = ls())
set.seed(22092008)
library(pacman)
# ggplot makes pretty graphs. dplyr is awesome. More later.
p_load(ggplot2, dplyr, crosstable, flextable, plyr, MASS)
# The goal is to generate some data from the relationship:
# Health <- shifter + b1*Gend + b2*Income + b3* Ins +  rnorm(n,mean=0,sd=1)
# We have to generate data for the variables (Gend, Income, Ins), a residual, and pick values
# for the coefficients. Let's assume a sample size of 1000.
n <- 1000 # Sample Size
mu <- c(0, 0,0) (means of my rhs variables)
# Lecture 4
# Let's do some testing of differences in means!
# and then some regression stuff.
# The repo also contains a knitr version of this file, which makes nice pdf files.....
rm(list = ls())
set.seed(22092008)
library(pacman)
# ggplot makes pretty graphs. dplyr is awesome. More later.
p_load(ggplot2, dplyr, crosstable, flextable, plyr, MASS)
# The goal is to generate some data from the relationship:
# Health <- shifter + b1*Gend + b2*Income + b3* Ins +  rnorm(n,mean=0,sd=1)
# We have to generate data for the variables (Gend, Income, Ins), a residual, and pick values
# for the coefficients. Let's assume a sample size of 1000.
n <- 1000 # Sample Size
mu <- c(0, 0,0) # (means of my rhs variables)
# Let's generate some data as if health insurance were randomized.
a <- 0.5 # Set to 0.5 as default
b <- 0.0 # Set to 0.1 as default
c <- 0.0 # Set to 0.8 as default
# Let's generate some data as if health insurance were not randomized.
# a <- 0.5 #Gend Income Covariance
# b <- 0.1 #Gend Insurance Covariance
# c <- 0.8 #Income Insurance
# Some betas (slope coefficients that only god sees) for later. She knows all!
b1 <-1 # Gend Beta
b2 <-5 # Income Beta
b3 <-3 #Insurance
shifter <- 30
# Now generate the data, given a cetain covariance matrix.
Sigma <- matrix(c(1, a, b, a, 1, c,b, c, 1), nrow=3)
data = mvrnorm(n, mu, Sigma, empirical=FALSE)
# Being lazy and generating some data columns from above.
Gender = data[, 1]  # standard normal (mu=0, sd=1)
Income = data[, 2]  # standard normal (mu=0, sd=1)
Insurance= data[, 3]  # standard normal (mu=0, sd=1)
# Gender and income should be binary
Ins <- Insurance>0
Gend <- Gender>0
cor(Ins,Gend)
cor(Ins,Income)
cor(Gend,Income)
# Lecture 4
# Let's do some testing of differences in means!
# and then some regression stuff.
# The repo also contains a knitr version of this file, which makes nice pdf files.....
rm(list = ls())
set.seed(22092008)
library(pacman)
# ggplot makes pretty graphs. dplyr is awesome. More later.
p_load(ggplot2, dplyr, crosstable, flextable, plyr, MASS)
# The goal is to generate some data from the relationship:
# Health <- shifter + b1*Gend + b2*Income + b3* Ins +  rnorm(n,mean=0,sd=1)
# We have to generate data for the variables (Gend, Income, Ins), a residual, and pick values
# for the coefficients. Let's assume a sample size of 1000.
n <- 1000 # Sample Size
mu <- c(0, 0,0) # (means of my rhs variables)
# Let's generate some data as if health insurance were randomized.
a <- 0.5 # Set to 0.5 as default
b <- 0.0 # Set to 0.1 as default
c <- 0.0 # Set to 0.8 as default
# Let's generate some data as if health insurance were not randomized.
# a <- 0.5 #Gend Income Covariance
# b <- 0.1 #Gend Insurance Covariance
# c <- 0.8 #Income Insurance
# Some betas (slope coefficients that only god sees) for later. She knows all!
b1 <-1 # Gend Beta
b2 <-5 # Income Beta
b3 <-3 #Insurance
shifter <- 30
# Now generate the data, given a cetain covariance matrix.
Sigma <- matrix(c(1, a, b, a, 1, c,b, c, 1), nrow=3)
data = mvrnorm(n, mu, Sigma, empirical=FALSE)
# Being lazy and generating some data columns from above.
Gender = data[, 1]  # standard normal (mu=0, sd=1)
Income = data[, 2]  # standard normal (mu=0, sd=1)
Insurance= data[, 3]  # standard normal (mu=0, sd=1)
# Gender and income should be binary
Ins <- Insurance>0
Gend <- Gender>0
cor(Ins,Gend)
cor(Ins,Income)
cor(Gend,Income)
# We are going to generate some arbitrary Health Index
Health <- shifter +  rnorm(n,mean=0,sd=1) + b1*Gend + b2*Income + b3* Ins
# Let's do a manual comparison of Health Across the Insured and Uninsured.
mydata <- data.frame(Income, Gend, Health, Ins)
# Calculate Means by Group (using ddply).
mu <- ddply(mydata, "Ins", summarise, grp.mean=mean(Health))
View(mu)
View(mu)
ggplot(mydata, aes(x=Health, color=Ins, fill=Ins)) +
scale_color_manual(values=c("#002676", "#FC9313")) +
scale_fill_manual(values=c("#002676", "#FC9313")) +
geom_histogram(alpha=0.1, position="identity", bins=50)+
geom_vline(data=mu, aes(xintercept=grp.mean, color=Ins),
linetype="dashed") +
labs(title="Health Outcomes by Insurance Status",x="Health Index", y = "Count")+
theme_classic()
M1 <- mean(mydata[Ins == 'TRUE', 'Health'])
M2 <- mean(mydata[Ins == 'FALSE', 'Health'])
n1 <- sum(Ins)
n2 <- n-n1
V1 <- var(mydata[Ins == 'TRUE', 'Health'])
V2 <- var(mydata[Ins == 'FALSE', 'Health'])
S <- sqrt((V1 / n1) + (V2 / n2))
statistic <- (M1 - M2 - 0) / S
print(statistic)
t_health <- t.test(Health ~ Ins)
View(t_health)
View(t_health)
print(t_health)
t_inc <- t.test(Income ~ Ins)
print(t_inc)
my_test_args=crosstable_test_args(show_method=FALSE)
ft1 <- crosstable(mydata,by="Ins", test=TRUE, funs=c(mean=mean),test_args=my_test_args) %>%
as_flextable()
print (ft1)
#Let's plot some data.
ggplot(airfares, aes(x=dist, y=fare)) +
geom_point(alpha=0.5, shape=16, fill="#002676", color="#002676", size=2)+
geom_smooth(method=lm, color="#FC9313")+
labs(title="Airfare by Distance",
x="Distance (Miles)", y = "Fare (US$)")+
theme_classic()
setwd("/Users/auffhammer/Library/CloudStorage/Dropbox/06_Teaching/MACSS/2025/COMPSS_201_2025/lecture_4")
airfares <- read.csv("airfares.csv")
#Let's plot some data.
ggplot(airfares, aes(x=dist, y=fare)) +
geom_point(alpha=0.5, shape=16, fill="#002676", color="#002676", size=2)+
geom_smooth(method=lm, color="#FC9313")+
labs(title="Airfare by Distance",
x="Distance (Miles)", y = "Fare (US$)")+
theme_classic()
#Let's run a regression.
planes <- lm(airfares$fare ~ airfares$dist)
# Let's record some residuals and join them to our data frame.
airfares$res <- planes$resid
#Let's make some nice looking regression output.
stargazer(planes, type='text', digits = 3, title = 'Linear Airfare Distance Regression', style = 'qje')
# Lecture 4
# Let's do some testing of differences in means!
# and then some regression stuff.
# The repo also contains a knitr version of this file, which makes nice pdf files.....
rm(list = ls())
set.seed(22092008)
library(pacman)
# ggplot makes pretty graphs. dplyr is awesome. More later.
p_load(ggplot2, dplyr, crosstable, flextable, plyr, MASS,stargazer)
# The goal is to generate some data from the relationship:
# Health <- shifter + b1*Gend + b2*Income + b3* Ins +  rnorm(n,mean=0,sd=1)
# We have to generate data for the variables (Gend, Income, Ins), a residual, and pick values
# for the coefficients. Let's assume a sample size of 1000.
n <- 1000 # Sample Size
mu <- c(0, 0,0) # (means of my rhs variables)
# Let's generate some data as if health insurance were randomized.
a <- 0.5 # Set to 0.5 as default
b <- 0.0 # Set to 0.1 as default
c <- 0.0 # Set to 0.8 as default
# Let's generate some data as if health insurance were not randomized.
# a <- 0.5 #Gend Income Covariance
# b <- 0.1 #Gend Insurance Covariance
# c <- 0.8 #Income Insurance
# Some betas (slope coefficients that only god sees) for later. She knows all!
b1 <-1 # Gend Beta
b2 <-5 # Income Beta
b3 <-3 #Insurance
shifter <- 30
# Now generate the data, given a cetain covariance matrix.
Sigma <- matrix(c(1, a, b, a, 1, c,b, c, 1), nrow=3)
data = mvrnorm(n, mu, Sigma, empirical=FALSE)
# Being lazy and generating some data columns from above.
Gender = data[, 1]  # standard normal (mu=0, sd=1)
Income = data[, 2]  # standard normal (mu=0, sd=1)
Insurance= data[, 3]  # standard normal (mu=0, sd=1)
# Gender and income should be binary
Ins <- Insurance>0
Gend <- Gender>0
cor(Ins,Gend)
cor(Ins,Income)
cor(Gend,Income)
# We are going to generate some arbitrary Health Index
Health <- shifter +  rnorm(n,mean=0,sd=1) + b1*Gend + b2*Income + b3* Ins
# Let's do a manual comparison of Health Across the Insured and Uninsured.
mydata <- data.frame(Income, Gend, Health, Ins)
# Calculate Means by Group (using ddply).
mu <- ddply(mydata, "Ins", summarise, grp.mean=mean(Health))
# Plot my health outcome by Insurance Status in Pretty Graph
ggplot(mydata, aes(x=Health, color=Ins, fill=Ins)) +
scale_color_manual(values=c("#002676", "#FC9313")) +
scale_fill_manual(values=c("#002676", "#FC9313")) +
geom_histogram(alpha=0.1, position="identity", bins=50)+
geom_vline(data=mu, aes(xintercept=grp.mean, color=Ins),
linetype="dashed") +
labs(title="Health Outcomes by Insurance Status",x="Health Index", y = "Count")+
theme_classic()
# Let's compare across treatment
# First - do this by hand. Difference in means. Unknown and unequal variances.
M1 <- mean(mydata[Ins == 'TRUE', 'Health'])
M2 <- mean(mydata[Ins == 'FALSE', 'Health'])
n1 <- sum(Ins)
n2 <- n-n1
V1 <- var(mydata[Ins == 'TRUE', 'Health'])
V2 <- var(mydata[Ins == 'FALSE', 'Health'])
S <- sqrt((V1 / n1) + (V2 / n2))
statistic <- (M1 - M2 - 0) / S
print(statistic)
t_health <- t.test(Health ~ Ins)
print(t_health)
t_inc <- t.test(Income ~ Ins)
print(t_inc)
my_test_args=crosstable_test_args(show_method=FALSE)
ft1 <- crosstable(mydata,by="Ins", test=TRUE, funs=c(mean=mean),test_args=my_test_args) %>%
as_flextable()
print (ft1)
# Now let's turn to some simple regression analysis.
# It is so simple, my teenager can do it.
# In fact, I checked and he can.
#That said, interpreting what it tells you is going to be the art form.
setwd("/Users/auffhammer/Library/CloudStorage/Dropbox/06_Teaching/MACSS/2025/COMPSS_201_2025/lecture_4")
airfares <- read.csv("airfares.csv")
#Let's plot some data.
ggplot(airfares, aes(x=dist, y=fare)) +
geom_point(alpha=0.5, shape=16, fill="#002676", color="#002676", size=2)+
geom_smooth(method=lm, color="#FC9313")+
labs(title="Airfare by Distance",
x="Distance (Miles)", y = "Fare (US$)")+
theme_classic()
#Let's run a regression.
planes <- lm(airfares$fare ~ airfares$dist)
# Let's record some residuals and join them to our data frame.
airfares$res <- planes$resid
#Let's make some nice looking regression output.
stargazer(planes, type='text', digits = 3, title = 'Linear Airfare Distance Regression', style = 'qje')
# Plot Residuals - Playing with colors (HEX Colors - official Cal!
# Also meesing with background and Axis Labels. )
ggplot(airfares, aes(x=dist, y=res)) +
geom_point(alpha=0.5, shape=16, fill="#002676", color="#002676", size=2)+
geom_smooth(method=lm, se=FALSE, color="#FC9313")+
labs(title="Airfare by Distance",
x="Distance (Miles)", y = "Residuals")+
theme_classic()
